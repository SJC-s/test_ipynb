import numpy as np
import faiss
import pickle
import os
import logging
from typing import List, Tuple, Optional
from sentence_transformers import SentenceTransformer

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class FaissKnowledgeBase:
    """
    Faissë¥¼ ì´ìš©í•œ í•œêµ­ì–´ ì§€ì‹ ë² ì´ìŠ¤ í´ë˜ìŠ¤
    ë²¡í„° ê²€ìƒ‰ì„ í†µí•œ ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰ ê¸°ëŠ¥ ì œê³µ
    """
    
    def __init__(self, model_name: str = 'jhgan/ko-sroberta-multitask', index_type: str = 'IVFFlat'):
        """
        ì´ˆê¸°í™”
        
        Args:
            model_name: ì‚¬ìš©í•  ì„ë² ë”© ëª¨ë¸ëª…
            index_type: Faiss ì¸ë±ìŠ¤ íƒ€ì… ('Flat', 'IVFFlat', 'HNSW')
        """
        self.model_name = model_name
        self.index_type = index_type
        self.model = None
        self.index = None
        self.data = []
        self.is_trained = False
        
        logger.info(f"FaissKnowledgeBase ì´ˆê¸°í™” - ëª¨ë¸: {model_name}, ì¸ë±ìŠ¤: {index_type}")
        
    def _load_model(self):
        """ì„ë² ë”© ëª¨ë¸ ë¡œë“œ"""
        if self.model is None:
            logger.info("ì„ë² ë”© ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤...")
            try:
                self.model = SentenceTransformer(self.model_name)
                logger.info("ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
            except Exception as e:
                logger.error(f"ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
                raise
    
    def _create_index(self, dimension: int, num_vectors: int):
        """
        Faiss ì¸ë±ìŠ¤ ìƒì„±
        
        Args:
            dimension: ë²¡í„° ì°¨ì›
            num_vectors: ë²¡í„° ê°œìˆ˜
        """
        logger.info(f"Faiss ì¸ë±ìŠ¤ ìƒì„± - íƒ€ì…: {self.index_type}, ì°¨ì›: {dimension}")
        
        if self.index_type == 'Flat':
            # ì •í™•í•œ ê²€ìƒ‰, ì‘ì€ ë°ì´í„°ì…‹ìš©
            self.index = faiss.IndexFlatL2(dimension)
            
        elif self.index_type == 'IVFFlat':
            # ë¹ ë¥¸ ê²€ìƒ‰, ì¤‘ê°„ í¬ê¸° ë°ì´í„°ì…‹ìš©
            nlist = min(max(int(np.sqrt(num_vectors)), 1), num_vectors)  # ì ì‘ì  nlist ì„¤ì •
            quantizer = faiss.IndexFlatL2(dimension)
            self.index = faiss.IndexIVFFlat(quantizer, dimension, nlist, faiss.METRIC_L2)
            
        elif self.index_type == 'HNSW':
            # ë§¤ìš° ë¹ ë¥¸ ê²€ìƒ‰, í° ë°ì´í„°ì…‹ìš©
            M = 16  # ì—°ê²° ìˆ˜
            self.index = faiss.IndexHNSWFlat(dimension, M)
            self.index.hnsw.efConstruction = 200
            self.index.hnsw.efSearch = 50
            
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì¸ë±ìŠ¤ íƒ€ì…: {self.index_type}")
    
    def add_documents(self, documents: List[str], batch_size: int = 100):
        """
        ë¬¸ì„œë“¤ì„ ì§€ì‹ ë² ì´ìŠ¤ì— ì¶”ê°€
        
        Args:
            documents: ì¶”ê°€í•  ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
            batch_size: ë°°ì¹˜ ì²˜ë¦¬ í¬ê¸°
        """
        self._load_model()
        
        if not documents:
            logger.warning("ì¶”ê°€í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        logger.info(f"{len(documents)}ê°œì˜ ë¬¸ì„œë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤...")
        
        # ê¸°ì¡´ ë°ì´í„°ì— ìƒˆ ë¬¸ì„œ ì¶”ê°€
        self.data.extend(documents)
        
        # ë°°ì¹˜ë³„ë¡œ ì„ë² ë”© ìƒì„±
        all_vectors = []
        for i in range(0, len(documents), batch_size):
            batch = documents[i:i + batch_size]
            logger.info(f"ë°°ì¹˜ {i//batch_size + 1} ì²˜ë¦¬ ì¤‘... ({len(batch)}ê°œ ë¬¸ì„œ)")
            
            try:
                batch_vectors = self.model.encode(batch, convert_to_numpy=True)
                all_vectors.append(batch_vectors)
            except Exception as e:
                logger.error(f"ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
                raise
        
        # ëª¨ë“  ë²¡í„° ê²°í•©
        db_vectors = np.vstack(all_vectors)
        
        # ì¸ë±ìŠ¤ê°€ ì—†ìœ¼ë©´ ìƒì„±
        if self.index is None:
            self._create_index(db_vectors.shape[1], len(self.data))
        
        # í•™ìŠµì´ í•„ìš”í•œ ì¸ë±ìŠ¤ íƒ€ì…ì¸ ê²½ìš° í•™ìŠµ ìˆ˜í–‰
        if hasattr(self.index, 'is_trained') and not self.index.is_trained:
            logger.info("ì¸ë±ìŠ¤ í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤...")
            self.index.train(db_vectors)
            logger.info("ì¸ë±ìŠ¤ í•™ìŠµ ì™„ë£Œ")
        
        # ë²¡í„° ì¶”ê°€
        self.index.add(db_vectors)
        self.is_trained = True
        
        logger.info(f"ì´ {self.index.ntotal}ê°œì˜ ë²¡í„°ê°€ ì¸ë±ìŠ¤ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    def search(self, query: str, k: int = 5, return_distances: bool = True) -> List[Tuple]:
        """
        ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰
        
        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            k: ë°˜í™˜í•  ê²°ê³¼ ìˆ˜
            return_distances: ê±°ë¦¬ ì •ë³´ í¬í•¨ ì—¬ë¶€
            
        Returns:
            ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸ [(ë¬¸ì„œ, ê±°ë¦¬), ...]
        """
        if not self.is_trained or self.index is None:
            logger.error("ì¸ë±ìŠ¤ê°€ í•™ìŠµë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. add_documents()ë¥¼ ë¨¼ì € í˜¸ì¶œí•˜ì„¸ìš”.")
            return []
        
        self._load_model()
        
        print(f"ğŸ” ì¿¼ë¦¬ ê²€ìƒ‰ ì¤‘: '{query}'")
        
        try:
            # ì¿¼ë¦¬ ë²¡í„°í™”
            query_vector = self.model.encode([query], convert_to_numpy=True)
            
            # ê²€ìƒ‰ ìˆ˜í–‰
            distances, indices = self.index.search(query_vector, k)
            
            # ê²°ê³¼ ì •ë¦¬
            results = []
            for idx, dist in zip(indices[0], distances[0]):
                if idx < len(self.data):  # ìœ íš¨í•œ ì¸ë±ìŠ¤ì¸ì§€ í™•ì¸
                    if return_distances:
                        results.append((self.data[idx], float(dist)))
                    else:
                        results.append(self.data[idx])
            
            print(f"âœ… {len(results)}ê°œì˜ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
            return results
            
        except Exception as e:
            print(f"âŒ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return []
    
    def save(self, filepath: str):
        """
        ì¸ë±ìŠ¤ì™€ ë°ì´í„° ì €ì¥
        
        Args:
            filepath: ì €ì¥í•  íŒŒì¼ ê²½ë¡œ (í™•ì¥ì ì œì™¸)
        """
        if not self.is_trained:
            logger.warning("í•™ìŠµë˜ì§€ ì•Šì€ ì¸ë±ìŠ¤ì…ë‹ˆë‹¤.")
            return
        
        try:
            # Faiss ì¸ë±ìŠ¤ ì €ì¥
            index_path = f"{filepath}.faiss"
            faiss.write_index(self.index, index_path)
            
            # ë©”íƒ€ë°ì´í„° ì €ì¥
            metadata = {
                'data': self.data,
                'model_name': self.model_name,
                'index_type': self.index_type,
                'is_trained': self.is_trained
            }
            
            metadata_path = f"{filepath}.pkl"
            with open(metadata_path, 'wb') as f:
                pickle.dump(metadata, f)
            
            logger.info(f"ì¸ë±ìŠ¤ ì €ì¥ ì™„ë£Œ: {index_path}, {metadata_path}")
            
        except Exception as e:
            logger.error(f"ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            raise
    
    def load(self, filepath: str):
        """
        ì €ì¥ëœ ì¸ë±ìŠ¤ì™€ ë°ì´í„° ë¡œë“œ
        
        Args:
            filepath: ë¡œë“œí•  íŒŒì¼ ê²½ë¡œ (í™•ì¥ì ì œì™¸)
        """
        try:
            # Faiss ì¸ë±ìŠ¤ ë¡œë“œ
            index_path = f"{filepath}.faiss"
            if not os.path.exists(index_path):
                raise FileNotFoundError(f"ì¸ë±ìŠ¤ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {index_path}")
            
            self.index = faiss.read_index(index_path)
            
            # ë©”íƒ€ë°ì´í„° ë¡œë“œ
            metadata_path = f"{filepath}.pkl"
            if not os.path.exists(metadata_path):
                raise FileNotFoundError(f"ë©”íƒ€ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {metadata_path}")
            
            with open(metadata_path, 'rb') as f:
                metadata = pickle.load(f)
            
            self.data = metadata['data']
            self.model_name = metadata['model_name']
            self.index_type = metadata['index_type']
            self.is_trained = metadata['is_trained']
            
            logger.info(f"ì¸ë±ìŠ¤ ë¡œë“œ ì™„ë£Œ: {self.index.ntotal}ê°œ ë²¡í„°")
            
        except Exception as e:
            logger.error(f"ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            raise
    
    def get_stats(self) -> dict:
        """ì¸ë±ìŠ¤ í†µê³„ ì •ë³´ ë°˜í™˜"""
        stats = {
            'total_vectors': self.index.ntotal if self.index else 0,
            'total_documents': len(self.data),
            'model_name': self.model_name,
            'index_type': self.index_type,
            'is_trained': self.is_trained
        }
        
        if self.index and hasattr(self.index, 'd'):
            stats['vector_dimension'] = self.index.d
            
        return stats


def main():
    """í…ŒìŠ¤íŠ¸ ë° ë°ëª¨ í•¨ìˆ˜"""
    # ìƒ˜í”Œ ë°ì´í„°
    sample_documents = [
        "ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ì •ë§ ì¢‹ë„¤ìš”.",
        "ì„œìš¸ì˜ ë§›ì§‘ì„ ì¶”ì²œí•´ì£¼ì„¸ìš”.",
        "íŒŒì´ì¬ìœ¼ë¡œ ì›¹ ìŠ¤í¬ë˜í•‘í•˜ëŠ” ë°©ë²•ì´ ê¶ê¸ˆí•©ë‹ˆë‹¤.",
        "ë‚´ì¼ ë¹„ê°€ ì˜¬ í™•ë¥ ì€ ì–¼ë§ˆë‚˜ ë˜ë‚˜ìš”?",
        "ì¸ê³µì§€ëŠ¥ ê¸°ìˆ ì˜ ë¯¸ë˜ ì „ë§ì— ëŒ€í•´ ì•Œë ¤ì£¼ì„¸ìš”.",
        "ê°€ì¥ ì¸ê¸° ìˆëŠ” í”„ë¡œê·¸ë˜ë° ì–¸ì–´ëŠ” ë¬´ì—‡ì¸ê°€ìš”?",
        "ìµœì‹  ìŠ¤ë§ˆíŠ¸í° ëª¨ë¸ ë¹„êµ ì •ë³´ë¥¼ ì°¾ê³  ìˆìŠµë‹ˆë‹¤.",
        "ê·¼ì²˜ì— ìˆëŠ” ê´œì°®ì€ ì¹´í˜ë¥¼ ì•Œë ¤ì£¼ì„¸ìš”.",
        "ë¨¸ì‹ ëŸ¬ë‹ ì•Œê³ ë¦¬ì¦˜ ì„±ëŠ¥ ë¹„êµ ë¶„ì„",
        "ë”¥ëŸ¬ë‹ ëª¨ë¸ ìµœì í™” ê¸°ë²•ë“¤",
        "ìì—°ì–´ ì²˜ë¦¬ ê¸°ìˆ ì˜ í˜„ì¬ì™€ ë¯¸ë˜",
        "ì»´í“¨í„° ë¹„ì „ ê¸°ìˆ  ë°œì „ ë™í–¥"
    ]
    
    # 1. ì§€ì‹ ë² ì´ìŠ¤ ìƒì„± ë° ë¬¸ì„œ ì¶”ê°€
    print("=== Faiss ì§€ì‹ ë² ì´ìŠ¤ í…ŒìŠ¤íŠ¸ ===")
    kb = FaissKnowledgeBase(index_type='IVFFlat')
    
    # ë¬¸ì„œ ì¶”ê°€
    kb.add_documents(sample_documents)
    
    # í†µê³„ ì¶œë ¥
    stats = kb.get_stats()
    print(f"\nğŸ“Š ì¸ë±ìŠ¤ í†µê³„:")
    for key, value in stats.items():
        print(f"  {key}: {value}")
    
    # 2. ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
    test_queries = [
        "ê°€ì¥ ì¢‹ì€ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ëŠ”?",
        "ì˜¤ëŠ˜ ë‚ ì”¨ ì–´ë•Œìš”?",
        "ë”¥ëŸ¬ë‹ì— ëŒ€í•´ ì•Œë ¤ì£¼ì„¸ìš”"
    ]
    
    print(f"\nğŸ” ê²€ìƒ‰ í…ŒìŠ¤íŠ¸:")
    for query in test_queries:
        print(f"\nì¿¼ë¦¬: '{query}'")
        results = kb.search(query, k=3)
        
        for i, (doc, distance) in enumerate(results, 1):
            print(f"  {i}. [{distance:.4f}] {doc}")
    
    # 3. ì €ì¥/ë¡œë“œ í…ŒìŠ¤íŠ¸
    print(f"\nğŸ’¾ ì €ì¥/ë¡œë“œ í…ŒìŠ¤íŠ¸:")
    save_path = "knowledge_base"
    
    # ì €ì¥
    kb.save(save_path)
    
    # ìƒˆë¡œìš´ ì¸ìŠ¤í„´ìŠ¤ë¡œ ë¡œë“œ
    kb_loaded = FaissKnowledgeBase()
    kb_loaded.load(save_path)
    
    # ë¡œë“œëœ ì¸ë±ìŠ¤ë¡œ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
    print(f"ë¡œë“œëœ ì¸ë±ìŠ¤ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸:")
    results = kb_loaded.search("í”„ë¡œê·¸ë˜ë° ì–¸ì–´", k=2)
    for i, (doc, distance) in enumerate(results, 1):
        print(f"  {i}. [{distance:.4f}] {doc}")
    
    print(f"\nâœ… ëª¨ë“  í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")


if __name__ == "__main__":
    main()