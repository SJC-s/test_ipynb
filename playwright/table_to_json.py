import asyncio
import json
from playwright.async_api import async_playwright
from bs4 import BeautifulSoup
from datetime import datetime
import re
import os
from urllib.parse import urlparse
import time

async def extract_tables_from_urls(urls_file="urls.txt"):
    """txt íŒŒì¼ì—ì„œ URL ëª©ë¡ì„ ì½ì–´ì„œ ê°ê°ì˜ í…Œì´ë¸”ì„ ì¶”ì¶œ"""
    # URL ëª©ë¡ ì½ê¸°
    urls = read_urls_from_file(urls_file)
    
    if not urls:
        print(f"âŒ {urls_file} íŒŒì¼ì´ ì—†ê±°ë‚˜ URLì´ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    print(f"ğŸ“‹ ì´ {len(urls)}ê°œì˜ URLì„ ì²˜ë¦¬í•©ë‹ˆë‹¤.")
    
    # ê²°ê³¼ ì €ì¥ìš© í´ë” ìƒì„±
    results_dir = "extracted_results"
    if not os.path.exists(results_dir):
        os.makedirs(results_dir)
    
    # ì „ì²´ ê²°ê³¼ ìš”ì•½
    all_results = []
    
    async with async_playwright() as p:
        browser = await p.chromium.launch()
        
        for i, url in enumerate(urls, 1):
            print(f"\n{'='*60}")
            print(f"ğŸŒ [{i}/{len(urls)}] {url} ì²˜ë¦¬ ì¤‘...")
            print(f"{'='*60}")
            
            try:
                result = await extract_tables_from_single_url(browser, url, results_dir)
                all_results.append(result)
                
                # ê° URL ì²˜ë¦¬ í›„ ì ì‹œ ëŒ€ê¸° (ì„œë²„ ë¶€í•˜ ë°©ì§€)
                await asyncio.sleep(1)
                
            except Exception as e:
                print(f"âŒ URL ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                error_result = {
                    "url": url,
                    "status": "error",
                    "error": str(e),
                    "timestamp": datetime.now().isoformat()
                }
                all_results.append(error_result)
        
        await browser.close()
    
    # ì „ì²´ ê²°ê³¼ ìš”ì•½ ì €ì¥
    summary_file = os.path.join(results_dir, "extraction_summary.json")
    with open(summary_file, 'w', encoding='utf-8') as f:
        json.dump({
            "total_urls": len(urls),
            "processed_at": datetime.now().isoformat(),
            "results": all_results
        }, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ‰ ëª¨ë“  URL ì²˜ë¦¬ ì™„ë£Œ!")
    print(f"ğŸ“ ê²°ê³¼ íŒŒì¼ë“¤ì´ '{results_dir}' í´ë”ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    print(f"ğŸ“Š ì „ì²´ ìš”ì•½: {summary_file}")

def read_urls_from_file(filename):
    """txt íŒŒì¼ì—ì„œ URL ëª©ë¡ì„ ì½ì–´ì˜¤ëŠ” í•¨ìˆ˜"""
    urls = []
    
    try:
        with open(filename, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                # ë¹ˆ ì¤„ì´ë‚˜ ì£¼ì„(#ìœ¼ë¡œ ì‹œì‘) ì œì™¸
                if line and not line.startswith('#'):
                    # httpë¡œ ì‹œì‘í•˜ì§€ ì•Šìœ¼ë©´ ì¶”ê°€
                    if not line.startswith(('http://', 'https://')):
                        line = 'https://' + line
                    urls.append(line)
    except FileNotFoundError:
        print(f"âŒ {filename} íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        # ê¸°ë³¸ URL íŒŒì¼ ìƒì„±
        create_sample_urls_file(filename)
        return []
    
    return urls

def create_sample_urls_file(filename):
    """ìƒ˜í”Œ URL íŒŒì¼ ìƒì„±"""
    sample_urls = [
        "# URL ëª©ë¡ íŒŒì¼ - í•œ ì¤„ì— í•˜ë‚˜ì”© ì…ë ¥í•˜ì„¸ìš”",
        "# ì£¼ì„ì€ #ìœ¼ë¡œ ì‹œì‘í•©ë‹ˆë‹¤",
        "",
        "https://kr.youme.com/sub_people/peopleopen.aspx?idx=204&tidx=0&dir=people",
        "https://tago.kr/service/ioniq6_monthly.htm",
        "# ì¶”ê°€ URLë“¤ì„ ì—¬ê¸°ì— ì…ë ¥í•˜ì„¸ìš”",
    ]
    
    with open(filename, 'w', encoding='utf-8') as f:
        f.write('\n'.join(sample_urls))
    
    print(f"ğŸ“ ìƒ˜í”Œ {filename} íŒŒì¼ì„ ìƒì„±í–ˆìŠµë‹ˆë‹¤. URLì„ ì¶”ê°€í•˜ê³  ë‹¤ì‹œ ì‹¤í–‰í•˜ì„¸ìš”.")

def get_safe_filename(url):
    """URLì—ì„œ ì•ˆì „í•œ íŒŒì¼ëª… ìƒì„±"""
    # URL íŒŒì‹±
    parsed = urlparse(url)
    domain = parsed.netloc
    path = parsed.path
    
    # íŠ¹ìˆ˜ë¬¸ì ì œê±° ë° ë³€í™˜
    safe_name = f"{domain}{path}".replace('/', '_').replace('\\', '_')
    safe_name = re.sub(r'[<>:"|?*]', '_', safe_name)
    safe_name = re.sub(r'[^\w\-_.]', '_', safe_name)
    safe_name = re.sub(r'_+', '_', safe_name).strip('_')
    
    # ê¸¸ì´ ì œí•œ (ìµœëŒ€ 100ì)
    if len(safe_name) > 100:
        safe_name = safe_name[:100]
    
    return safe_name

async def extract_tables_from_single_url(browser, url, results_dir):
    """ë‹¨ì¼ URLì—ì„œ í…Œì´ë¸” ì¶”ì¶œ"""
    page = await browser.new_page()
    
    try:
        # ì›¹í˜ì´ì§€ ë¡œë“œ
        await page.goto(url, timeout=30000)
        await page.wait_for_load_state('networkidle', timeout=30000)
        
        # HTML ë‚´ìš© ê°€ì ¸ì˜¤ê¸°
        html_content = await page.content()
        
        # BeautifulSoupìœ¼ë¡œ HTML íŒŒì‹±
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # ë‹¤ì–‘í•œ í…Œì´ë¸” í˜•ì‹ ì°¾ê¸°
        table_candidates = find_table_candidates(soup)
        
        # ë°ì´í„° êµ¬ì¡° ìƒì„±
        extracted_data = {
            "extraction_info": {
                "url": url,
                "timestamp": datetime.now().isoformat(),
                "total_tables": len(table_candidates),
                "page_title": soup.title.string if soup.title else "ì œëª© ì—†ìŒ"
            },
            "tables": []
        }
        
        print(f"ğŸ“Š ì´ {len(table_candidates)}ê°œì˜ í…Œì´ë¸” í˜•ì‹ì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤.")
        
        # ê° í…Œì´ë¸” ì²˜ë¦¬
        for i, table_info in enumerate(table_candidates):
            print(f"  â””â”€ í…Œì´ë¸” {i+1}: {table_info['type']}")
            
            table_data = extract_table_data(table_info, i + 1)
            extracted_data["tables"].append(table_data)
        
        # íŒŒì¼ëª… ìƒì„±
        safe_filename = get_safe_filename(url)
        json_filename = f"{safe_filename}.json"
        json_filepath = os.path.join(results_dir, json_filename)
        
        # JSON íŒŒì¼ ì €ì¥
        with open(json_filepath, 'w', encoding='utf-8') as f:
            json.dump(extracted_data, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… ì €ì¥ ì™„ë£Œ: {json_filename}")
        
        return {
            "url": url,
            "status": "success",
            "filename": json_filename,
            "tables_found": len(table_candidates),
            "page_title": extracted_data["extraction_info"]["page_title"]
        }
        
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        raise e
    finally:
        await page.close()

async def extract_tables_to_json():
    """ì›¹í˜ì´ì§€ì—ì„œ ë‹¤ì–‘í•œ í˜•íƒœì˜ í…Œì´ë¸” ë°ì´í„°ë¥¼ ì¶”ì¶œí•˜ì—¬ JSONìœ¼ë¡œ ì €ì¥"""
    async with async_playwright() as p:
        browser = await p.chromium.launch()
        page = await browser.new_page()
        
        try:
            # ì›¹í˜ì´ì§€ ë¡œë“œ
            url = "https://kr.youme.com/sub_people/peopleopen.aspx?idx=204&tidx=0&dir=people"
            await page.goto(url)
            await page.wait_for_load_state('networkidle')
            
            # HTML ë‚´ìš© ê°€ì ¸ì˜¤ê¸°
            html_content = await page.content()
            
            # BeautifulSoupìœ¼ë¡œ HTML íŒŒì‹±
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # ë‹¤ì–‘í•œ í…Œì´ë¸” í˜•ì‹ ì°¾ê¸°
            table_candidates = find_table_candidates(soup)
            
            # ì „ì²´ ë°ì´í„° êµ¬ì¡°
            extracted_data = {
                "extraction_info": {
                    "url": url,
                    "timestamp": datetime.now().isoformat(),
                    "total_tables": len(table_candidates)
                },
                "tables": []
            }
            
            print(f"ì´ {len(table_candidates)}ê°œì˜ í…Œì´ë¸” í˜•ì‹ì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤.")
            
            for i, table_info in enumerate(table_candidates):
                print(f"\n=== í…Œì´ë¸” {i+1} ì²˜ë¦¬ ì¤‘ ===")
                print(f"ìœ í˜•: {table_info['type']}")
                
                table_data = extract_table_data(table_info, i + 1)
                extracted_data["tables"].append(table_data)
                
                print(f"ì œëª©: {table_data['title']}")
                print(f"  - í—¤ë” í–‰: {len(table_data['headers'])}")
                print(f"  - ë°ì´í„° í–‰: {len(table_data['rows'])}")
                print(f"  - ì»¬ëŸ¼ ìˆ˜: {len(table_data['rows'][0]) if table_data['rows'] else 0}")
                print(f"  - í…Œì´ë¸” ìœ í˜•: {table_data['table_type']}")
            
            # JSON íŒŒì¼ë¡œ ì €ì¥
            with open('extracted_tables.json', 'w', encoding='utf-8') as f:
                json.dump(extracted_data, f, ensure_ascii=False, indent=2)
            
            print(f"\nâœ… ëª¨ë“  í…Œì´ë¸” ë°ì´í„°ê°€ 'extracted_tables.json'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!")
            
            return extracted_data
            
        except Exception as e:
            print(f"ì˜¤ë¥˜ ë°œìƒ: {e}")
            return None
        finally:
            await browser.close()

def find_table_candidates(soup):
    """ë‹¤ì–‘í•œ í˜•íƒœì˜ í…Œì´ë¸” í›„ë³´ë¥¼ ì°¾ëŠ” í•¨ìˆ˜"""
    candidates = []
    
    # 1. í‘œì¤€ HTML í…Œì´ë¸”
    html_tables = soup.find_all('table')
    for table in html_tables:
        candidates.append({
            'type': 'html_table',
            'element': table,
            'confidence': 1.0
        })
    
    # 2. div ê¸°ë°˜ í…Œì´ë¸” (CSS Grid/Flexbox)
    div_tables = soup.find_all('div', class_=re.compile(r'table|grid|data-table', re.I))
    for div in div_tables:
        if has_table_structure(div):
            candidates.append({
                'type': 'div_table',
                'element': div,
                'confidence': 0.8
            })
    
    # 3. ul/ol ê¸°ë°˜ ë¦¬ìŠ¤íŠ¸ í…Œì´ë¸”
    list_tables = soup.find_all(['ul', 'ol'], class_=re.compile(r'table|list-table|data-list', re.I))
    for list_elem in list_tables:
        if has_list_table_structure(list_elem):
            candidates.append({
                'type': 'list_table',
                'element': list_elem,
                'confidence': 0.7
            })
    
    # 4. dl (Definition List) ê¸°ë°˜ í…Œì´ë¸”
    dl_tables = soup.find_all('dl')
    for dl in dl_tables:
        if has_dl_table_structure(dl):
            candidates.append({
                'type': 'dl_table',
                'element': dl,
                'confidence': 0.6
            })
    
    # 5. ë°˜ë³µë˜ëŠ” div íŒ¨í„´ (ì¹´ë“œ í˜•íƒœ)
    card_patterns = find_card_patterns(soup)
    for pattern in card_patterns:
        candidates.append({
            'type': 'card_table',
            'element': pattern,
            'confidence': 0.5
        })
    
    # ì‹ ë¢°ë„ ìˆœìœ¼ë¡œ ì •ë ¬
    candidates.sort(key=lambda x: x['confidence'], reverse=True)
    
    return candidates

def has_table_structure(div):
    """div ìš”ì†Œê°€ í…Œì´ë¸” êµ¬ì¡°ë¥¼ ê°€ì§€ëŠ”ì§€ í™•ì¸"""
    # í–‰ì„ ë‚˜íƒ€ë‚´ëŠ” í•˜ìœ„ ìš”ì†Œë“¤ì´ ìˆëŠ”ì§€ í™•ì¸
    rows = div.find_all('div', class_=re.compile(r'row|tr|table-row', re.I))
    if len(rows) >= 2:  # ìµœì†Œ 2ê°œ í–‰
        # ê° í–‰ì— ì…€ë“¤ì´ ìˆëŠ”ì§€ í™•ì¸
        for row in rows[:3]:  # ì²˜ìŒ 3ê°œ í–‰ë§Œ í™•ì¸
            cells = row.find_all(['div', 'span'], class_=re.compile(r'cell|td|col', re.I))
            if len(cells) >= 2:  # ìµœì†Œ 2ê°œ ì…€
                return True
    return False

def has_list_table_structure(list_elem):
    """ë¦¬ìŠ¤íŠ¸ ìš”ì†Œê°€ í…Œì´ë¸” êµ¬ì¡°ë¥¼ ê°€ì§€ëŠ”ì§€ í™•ì¸"""
    items = list_elem.find_all('li')
    if len(items) >= 2:
        # ê° í•­ëª©ì´ ì¼ì •í•œ íŒ¨í„´ì„ ê°€ì§€ëŠ”ì§€ í™•ì¸
        first_item_structure = get_element_structure(items[0])
        similar_count = sum(1 for item in items[1:3] 
                          if is_similar_structure(first_item_structure, get_element_structure(item)))
        return similar_count >= 1
    return False

def has_dl_table_structure(dl):
    """dl ìš”ì†Œê°€ í…Œì´ë¸” êµ¬ì¡°ë¥¼ ê°€ì§€ëŠ”ì§€ í™•ì¸"""
    dt_elements = dl.find_all('dt')
    dd_elements = dl.find_all('dd')
    # dtì™€ ddê°€ ìŒì„ ì´ë£¨ëŠ”ì§€ í™•ì¸
    return len(dt_elements) >= 2 and len(dd_elements) >= 2

def find_card_patterns(soup):
    """ë°˜ë³µë˜ëŠ” ì¹´ë“œ íŒ¨í„´ì„ ì°¾ëŠ” í•¨ìˆ˜"""
    patterns = []
    
    # ê°™ì€ í´ë˜ìŠ¤ë¥¼ ê°€ì§„ ë°˜ë³µ ìš”ì†Œë“¤ ì°¾ê¸°
    potential_cards = soup.find_all('div', class_=True)
    class_counts = {}
    
    for card in potential_cards:
        classes = ' '.join(card.get('class', []))
        if classes and len(classes.split()) <= 3:  # ë„ˆë¬´ ë³µì¡í•œ í´ë˜ìŠ¤ëŠ” ì œì™¸
            class_counts[classes] = class_counts.get(classes, 0) + 1
    
    # 3ê°œ ì´ìƒ ë°˜ë³µë˜ëŠ” íŒ¨í„´ ì°¾ê¸°
    for class_name, count in class_counts.items():
        if count >= 3:
            cards = soup.find_all('div', class_=class_name.split())
            if all(has_structured_content(card) for card in cards[:3]):
                patterns.append({
                    'class': class_name,
                    'elements': cards,
                    'count': count
                })
    
    return patterns

def has_structured_content(element):
    """ìš”ì†Œê°€ êµ¬ì¡°í™”ëœ ë‚´ìš©ì„ ê°€ì§€ëŠ”ì§€ í™•ì¸"""
    # í…ìŠ¤íŠ¸ ë…¸ë“œì™€ í•˜ìœ„ ìš”ì†Œë“¤ì˜ ë¹„ìœ¨ í™•ì¸
    text_content = element.get_text(strip=True)
    child_elements = element.find_all(True)
    
    return len(text_content) > 10 and len(child_elements) >= 2

def get_element_structure(element):
    """ìš”ì†Œì˜ êµ¬ì¡°ë¥¼ ë¶„ì„í•˜ì—¬ íŒ¨í„´ ë°˜í™˜"""
    return {
        'tag_count': len(element.find_all(True)),
        'text_length': len(element.get_text(strip=True)),
        'child_tags': [child.name for child in element.find_all(True, recursive=False)]
    }

def is_similar_structure(struct1, struct2):
    """ë‘ êµ¬ì¡°ê°€ ìœ ì‚¬í•œì§€ í™•ì¸"""
    tag_diff = abs(struct1['tag_count'] - struct2['tag_count'])
    text_diff = abs(struct1['text_length'] - struct2['text_length'])
    
    return tag_diff <= 2 and text_diff <= 50

def extract_table_data(table_info, table_id):
    """í…Œì´ë¸” ì •ë³´ì—ì„œ ë°ì´í„°ë¥¼ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜"""
    table_type = table_info['type']
    element = table_info['element']
    
    if table_type == 'html_table':
        return extract_html_table(element, table_id)
    elif table_type == 'div_table':
        return extract_div_table(element, table_id)
    elif table_type == 'list_table':
        return extract_list_table(element, table_id)
    elif table_type == 'dl_table':
        return extract_dl_table(element, table_id)
    elif table_type == 'card_table':
        return extract_card_table(table_info, table_id)
    else:
        return create_empty_table(table_id)

def extract_html_table(table, table_id):
    """í‘œì¤€ HTML í…Œì´ë¸”ì—ì„œ ë°ì´í„° ì¶”ì¶œ"""
    # ê¸°ì¡´ ë¡œì§ ì‚¬ìš©
    caption = table.find('caption')
    table_title = caption.get_text(strip=True) if caption else f"í…Œì´ë¸” {table_id}"
    
    table_data = {
        "table_id": table_id,
        "title": table_title,
        "headers": [],
        "rows": [],
        "table_type": "html_table"
    }
    
    # í—¤ë” ì¶”ì¶œ
    thead = table.find('thead')
    if thead:
        header_rows = thead.find_all('tr')
        for header_row in header_rows:
            headers = []
            for th in header_row.find_all(['th', 'td']):
                headers.append(th.get_text(strip=True))
            table_data["headers"].append(headers)
    
    # ë°ì´í„° í–‰ ì¶”ì¶œ
    tbody = table.find('tbody')
    if tbody:
        rows = tbody.find_all('tr')
    else:
        rows = table.find_all('tr')
        if not thead and rows:
            first_row = rows[0]
            if first_row.find('th'):
                headers = []
                for th in first_row.find_all(['th', 'td']):
                    headers.append(th.get_text(strip=True))
                table_data["headers"].append(headers)
                rows = rows[1:]
    
    # í–‰ í—¤ë” ê°ì§€
    has_row_headers = False
    if rows:
        th_first_count = sum(1 for row in rows 
                           if row.find(['td', 'th']) and row.find(['td', 'th']).name == 'th')
        has_row_headers = (th_first_count / len(rows)) >= 0.8
    
    table_data["has_row_headers"] = has_row_headers
    
    # ë°ì´í„° í–‰ ì²˜ë¦¬
    for row in rows:
        row_data = []
        all_cells = row.find_all(['td', 'th'])
        for cell in all_cells:
            cell_text = cell.get_text(strip=True)
            row_data.append(cell_text)
        table_data["rows"].append(row_data)
    
    return table_data

def extract_div_table(div, table_id):
    """div ê¸°ë°˜ í…Œì´ë¸”ì—ì„œ ë°ì´í„° ì¶”ì¶œ"""
    table_data = {
        "table_id": table_id,
        "title": f"Div í…Œì´ë¸” {table_id}",
        "headers": [],
        "rows": [],
        "table_type": "div_table"
    }
    
    # í—¤ë” ì°¾ê¸°
    header_row = div.find('div', class_=re.compile(r'header|thead', re.I))
    if header_row:
        headers = []
        header_cells = header_row.find_all(['div', 'span'], class_=re.compile(r'cell|col', re.I))
        for cell in header_cells:
            headers.append(cell.get_text(strip=True))
        table_data["headers"].append(headers)
    
    # ë°ì´í„° í–‰ ì°¾ê¸°
    rows = div.find_all('div', class_=re.compile(r'row|tr', re.I))
    for row in rows:
        if header_row and row == header_row:
            continue
        
        row_data = []
        cells = row.find_all(['div', 'span'], class_=re.compile(r'cell|col', re.I))
        for cell in cells:
            row_data.append(cell.get_text(strip=True))
        
        if row_data:
            table_data["rows"].append(row_data)
    
    return table_data

def extract_list_table(list_elem, table_id):
    """ë¦¬ìŠ¤íŠ¸ ê¸°ë°˜ í…Œì´ë¸”ì—ì„œ ë°ì´í„° ì¶”ì¶œ"""
    table_data = {
        "table_id": table_id,
        "title": f"ë¦¬ìŠ¤íŠ¸ í…Œì´ë¸” {table_id}",
        "headers": [],
        "rows": [],
        "table_type": "list_table"
    }
    
    items = list_elem.find_all('li')
    for item in items:
        row_data = []
        
        # ê° í•­ëª©ì—ì„œ êµ¬ì¡°í™”ëœ ë°ì´í„° ì¶”ì¶œ
        labels = item.find_all(['strong', 'b', 'label'])
        values = item.find_all(['span', 'div'])
        
        if labels and values:
            for label, value in zip(labels, values):
                row_data.extend([label.get_text(strip=True), value.get_text(strip=True)])
        else:
            # ë‹¨ìˆœ í…ìŠ¤íŠ¸ ë¶„í• 
            text = item.get_text(strip=True)
            if ':' in text:
                parts = text.split(':', 1)
                row_data.extend([part.strip() for part in parts])
            else:
                row_data.append(text)
        
        if row_data:
            table_data["rows"].append(row_data)
    
    return table_data

def extract_dl_table(dl, table_id):
    """dl ê¸°ë°˜ í…Œì´ë¸”ì—ì„œ ë°ì´í„° ì¶”ì¶œ"""
    table_data = {
        "table_id": table_id,
        "title": f"ì •ì˜ ëª©ë¡ í…Œì´ë¸” {table_id}",
        "headers": ["í•­ëª©", "ì„¤ëª…"],
        "rows": [],
        "table_type": "dl_table"
    }
    
    dt_elements = dl.find_all('dt')
    dd_elements = dl.find_all('dd')
    
    for dt, dd in zip(dt_elements, dd_elements):
        row_data = [
            dt.get_text(strip=True),
            dd.get_text(strip=True)
        ]
        table_data["rows"].append(row_data)
    
    return table_data

def extract_card_table(table_info, table_id):
    """ì¹´ë“œ íŒ¨í„´ì—ì„œ í…Œì´ë¸” ë°ì´í„° ì¶”ì¶œ"""
    pattern = table_info['element']
    elements = pattern['elements']
    
    table_data = {
        "table_id": table_id,
        "title": f"ì¹´ë“œ í…Œì´ë¸” {table_id}",
        "headers": [],
        "rows": [],
        "table_type": "card_table"
    }
    
    # ì²« ë²ˆì§¸ ì¹´ë“œì—ì„œ í—¤ë” êµ¬ì¡° ì¶”ì¶œ
    if elements:
        first_card = elements[0]
        headers = []
        
        # ë¼ë²¨ë“¤ì„ í—¤ë”ë¡œ ì‚¬ìš©
        labels = first_card.find_all(['strong', 'b', 'label', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6'])
        for label in labels:
            headers.append(label.get_text(strip=True))
        
        if headers:
            table_data["headers"].append(headers)
    
    # ê° ì¹´ë“œì—ì„œ ë°ì´í„° ì¶”ì¶œ
    for card in elements:
        row_data = []
        
        # êµ¬ì¡°í™”ëœ ë°ì´í„° ì¶”ì¶œ
        data_elements = card.find_all(['span', 'div', 'p'])
        for elem in data_elements:
            text = elem.get_text(strip=True)
            if text and len(text) > 0:
                row_data.append(text)
        
        if row_data:
            table_data["rows"].append(row_data)
    
    return table_data

def create_empty_table(table_id):
    """ë¹ˆ í…Œì´ë¸” ìƒì„±"""
    return {
        "table_id": table_id,
        "title": f"í…Œì´ë¸” {table_id}",
        "headers": [],
        "rows": [],
        "table_type": "unknown"
    }

# ì‹¤í–‰
if __name__ == "__main__":
    import sys
    
    # ëª…ë ¹í–‰ ì¸ì í™•ì¸
    if len(sys.argv) > 1:
        if sys.argv[1] == "multi":
            # ë‹¤ì¤‘ URL ëª¨ë“œ
            urls_file = sys.argv[2] if len(sys.argv) > 2 else "urls.txt"
            result = asyncio.run(extract_tables_from_urls(urls_file))
        elif sys.argv[1] == "single":
            # ë‹¨ì¼ URL ëª¨ë“œ
            result = asyncio.run(extract_tables_to_json())
        else:
            print("âŒ ì˜ëª»ëœ ì¸ìì…ë‹ˆë‹¤.")
            print("ì‚¬ìš©ë²•:")
            print("  python table_to_json.py multi [urls.txt]  # ë‹¤ì¤‘ URL ì²˜ë¦¬")
            print("  python table_to_json.py single           # ë‹¨ì¼ URL ì²˜ë¦¬")
    else:
        # ê¸°ë³¸ê°’: urls.txt íŒŒì¼ì´ ìˆìœ¼ë©´ ë‹¤ì¤‘ ëª¨ë“œ, ì—†ìœ¼ë©´ ë‹¨ì¼ ëª¨ë“œ
        if os.path.exists("urls.txt"):
            print("ğŸ“‹ urls.txt íŒŒì¼ì´ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤. ë‹¤ì¤‘ URL ëª¨ë“œë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤.")
            print("ğŸ’¡ ë‹¨ì¼ URL ëª¨ë“œë¡œ ì‹¤í–‰í•˜ë ¤ë©´: python table_to_json.py single")
            result = asyncio.run(extract_tables_from_urls())
        else:
            print("ğŸŒ ë‹¨ì¼ URL ëª¨ë“œë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤.")
            print("ğŸ’¡ ë‹¤ì¤‘ URL ëª¨ë“œë¡œ ì‹¤í–‰í•˜ë ¤ë©´: python table_to_json.py multi")
            result = asyncio.run(extract_tables_to_json())
    
    if result:
        print(f"\nğŸ‰ ì²˜ë¦¬ ì™„ë£Œ!") 