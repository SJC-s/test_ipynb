import asyncio
import json
import os
from datetime import datetime
from dotenv import load_dotenv
from langgraph.prebuilt import create_react_agent
from langchain_openai import ChatOpenAI
from langchain_core.messages import SystemMessage


load_dotenv()

# SystemMessage í´ë˜ìŠ¤ ì •ì˜
class SystemMessage:
    def __init__(self, content):
        self.content = content

# get_agent_with_rag_ingredient_v3 í•¨ìˆ˜ ì •ì˜
async def get_agent_with_rag_ingredient_v3(
    query: str,
):
    today = datetime.now().strftime("%Y-%m-%d")
    weekday_korean = ["ì›”ìš”ì¼", "í™”ìš”ì¼", "ìˆ˜ìš”ì¼", "ëª©ìš”ì¼", "ê¸ˆìš”ì¼", "í† ìš”ì¼", "ì¼ìš”ì¼"]
    current_weekday = weekday_korean[datetime.now().weekday()]

    answer_prompt = SystemMessage(
        content = f"""
You are an interactive chatbot that answers your questions, Multilingual assistants who answer user questions with searches from multiple tools (RAG, web, image, etc.).  
Today is {today}, {current_weekday}.  

query: {query}  

Strictly follow these rules:  
1. Use available tools to generate a response to the user's question.  
2. Detect the user's input language and always reply in the same language.  
3. When expressing time ranges or any other ranges, include a space on both sides of the tilde (~) symbol, for example: 07:00 ~ 16:00    
"""
    )

    logging_prompt = SystemMessage(
        content = f"""
ë„ˆëŠ” ì¶œì²˜ ì •ë³´ë¥¼ ìƒì„±í•˜ëŠ” ì—ì´ì „íŠ¸ì•¼. 

ğŸ”’ í•„ìˆ˜ ì†ŒìŠ¤ ë¡œê¹… - ì¤‘ìš”í•œ ê·œì¹™:  
retrievers ë¦¬ìŠ¤íŠ¸ë¥¼ ì°¸ê³ í•˜ì—¬, ê° ê²€ìƒ‰ ê²°ê³¼ì˜ title(ëŒ€í‘œ ì œëª©)ê³¼ snippet(ëŒ€í‘œ ì¸ìš©ë¬¸)ì„ ì§ì ‘ ìƒì„±í•˜ë¼.
ë°˜ë“œì‹œ ë‹µë³€ê³¼ í•¨ê»˜ ì¶œì²˜ ì •ë³´ë¥¼ ëª…í™•í•˜ê²Œ í¬í•¨í•˜ë¼.
title, snippetì€ retrieversì—ì„œ ì§ì ‘ ì¶”ì¶œí•˜ì§€ ë§ê³ , LLMì´ ìš”ì•½/ìƒì„±í•´ì„œ ë„£ì–´ë¼.
í•„ë“œì—ì„œ [ì¶œì²˜:], [Chunk_number:], [Title:], [Chunk:]ì™€ ê°™ì€ ë©”íƒ€ ì •ë³´ë¥¼ ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ë§ˆì‹­ì‹œì˜¤.  
ìŠ¤ë‹ˆí«ì„ ì¶”ì¶œí•  ë•Œ ì¸ìš©ë¬¸ì„ 200ìë¡œ ì œí•œí•˜ê³  ì¸ìš©ëœ êµ¬ì ˆ ì£¼ë³€ì— ìµœëŒ€ 100ìì˜ ë¬¸ë§¥ì„ í¬í•¨ì‹œí‚µë‹ˆë‹¤. ì›ë¬¸ì´ 200ìë¥¼ ì´ˆê³¼í•˜ëŠ” ê²½ìš°, ì´ì— ë”°ë¼ ì˜ë¼ë‚´ê³  ì•ì²¨ë¶€/ì²¨ë¶€í•©ë‹ˆë‹¤.  

ì œëª© ë° ìŠ¤ë‹ˆí« í•„ë“œì— ëŒ€í•œ ğŸ”’ ì¤‘ìš”í•œ í˜•ì‹ ê·œì¹™:  
   â†’ title: 20ì ì´í•˜ì´ì–´ì•¼ í•˜ë©°, ì œëª©, ì¶œì²˜, ì²­í¬_ë„˜ë²„ ë“±ê³¼ ê°™ì€ ë©”íƒ€ë°ì´í„° ì ‘ë‘ì‚¬ë¥¼ ì œê±°í•´ì•¼ í•©ë‹ˆë‹¤.  
   â†’ snippet: í…ìŠ¤íŠ¸ë§Œ ì •ë¦¬í•˜ê³ , í•œêµ­ì–´, ì˜ì–´, ìˆ«ì, ê³µë°±, ë§ˆì¹¨í‘œ, ì‰¼í‘œë¥¼ ì œì™¸í•œ ëª¨ë“  ë©”íƒ€ë°ì´í„° ë§ˆì»¤ì™€ íŠ¹ìˆ˜ ë¬¸ìë¥¼ ì œê±°í•©ë‹ˆë‹¤.  
   â†’ ì œëª©ê³¼ ìŠ¤ë‹ˆí« ëª¨ë‘: ì½œë¡ , ê´„í˜¸, ë°±ìŠ¬ë˜ì‹œ ë˜ëŠ” ê´„í˜¸ëŠ” ì ˆëŒ€ í¬í•¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.  

ğŸš¨ JSON ì•ˆì „ ê·œì • - ëª¨ë“  ë°˜í™˜ëœ í…ìŠ¤íŠ¸ í•„ë“œì— í•„ìˆ˜:  
   â†’ ì½œë¡ , ë‹¨ì¼ ë”°ì˜´í‘œ, ì´ì¤‘ ë”°ì˜´í‘œ, ë°±ìŠ¬ë˜ì‹œ, ê³±ìŠ¬ ê´„í˜¸, ì‚¬ê° ê´„í˜¸ ë“± JSONì„ ê¹¨ëŠ” ë¬¸ìëŠ” ì ˆëŒ€ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.  
   â†’ ë”°ì˜´í‘œë¥¼ ì œê±°í•˜ê±°ë‚˜ ê³µë°±ìœ¼ë¡œ ëŒ€ì²´í•˜ê±°ë‚˜ ì™„ì „íˆ ìƒëµí•©ë‹ˆë‹¤.  
   â†’ ì½œë¡ ì„ ê³µë°±ì´ë‚˜ í•˜ì´í”ˆìœ¼ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.  
   â†’ ëª¨ë“  ë°±ìŠ¬ë˜ì‹œ, ê³±ìŠ¬ ë¸Œë ˆì´ìŠ¤, ì‚¬ê° ê´„í˜¸ë¥¼ ì œê±°í•©ë‹ˆë‹¤.  
   â†’ í…ìŠ¤íŠ¸ì— ì´ëŸ¬í•œ ë¬¸ìê°€ í¬í•¨ë˜ì–´ ìˆëŠ” ê²½ìš°, ë°˜í™˜í•˜ê¸° ì „ì— ì •ë¦¬í•˜ì„¸ìš”.  

        """
    )

    return answer_prompt, logging_prompt

# ë‹µë³€ ìƒì„±(ìŠ¤íŠ¸ë¦¬ë°) í•¨ìˆ˜ ì •ì˜ (OpenAI API í•„ìš”)
import openai

async def ai_search_answer_streaming(llm, prompt, retrievers):
    client = openai.OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
    loop = asyncio.get_event_loop()
    try:
        def sync_stream():
            return client.chat.completions.create(
                model=llm,
                messages=[
                    {"role": "system", "content": prompt},
                    {"role": "user", "content": retrievers}
                ],
                stream=True,
                temperature=0.2
            )
        response = await loop.run_in_executor(None, sync_stream)
        for chunk in response:
            delta = chunk.choices[0].delta
            if delta and hasattr(delta, "content"):
                yield {"content": delta.content, "done": False, "description": "main_content"}
    except Exception as e:
        print(f"Error during streaming: {e}")

# retrivers.txt íŒŒì¼ì—ì„œ ê²€ìƒ‰ ê²°ê³¼ ë¶ˆëŸ¬ì˜¤ê¸°
def load_retrievers_from_txt(filepath="retrivers.json"):
    with open(filepath, "r", encoding="utf-8") as f:
        content = f.read().strip()
        if not content:
            raise ValueError("retrivers.txt íŒŒì¼ì´ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")
        try:
            retrievers = json.loads(content)
        except json.JSONDecodeError as e:
            print("retrivers.txt íŒŒì¼ì˜ JSON í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            raise e
    return retrievers

# í…ŒìŠ¤íŠ¸ ì‹¤í–‰ í•¨ìˆ˜
async def test_answer_and_source():
    retrievers = load_retrievers_from_txt()

    llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=0, openai_api_key=os.getenv("OPENAI_API_KEY"))

    query = "ë‚˜ë¼ì›ì‹œìŠ¤í…œì˜ ì—°í˜ì„ ì•Œë ¤ì¤˜"

    answer_prompt, logging_prompt = await get_agent_with_rag_ingredient_v3(
        query=query,
    )

    # ReAct agentì—ì„œ tool ì—†ì´ ë™ì‘
    graph_v3 = create_react_agent(
        llm,
        tools=[],
        prompt = logging_prompt.content
    )

    print("\n=== ì¶œì²˜ ì •ë³´ ìƒì„± ë° ë‹µë³€ ìƒì„± í…ŒìŠ¤íŠ¸ ===")
    # LLMì´ retrieversë¥¼ ë³´ê³  ì§ì ‘ title, snippetì„ ìƒì„±í•´ì„œ ë‹µë³€ì— í¬í•¨í•˜ë„ë¡ ìœ ë„
    result = await graph_v3.ainvoke(
        {
            "messages": [
                {"role": "user", "content": str(retrievers) + "ì˜ titleê³¼ snippetë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ìƒì„±í•´ì¤˜"}
            ],
        }
    )
    print(result.get["AIMessage"])

    print("=== ë‹µë³€ ìƒì„±(ìŠ¤íŠ¸ë¦¬ë°) í…ŒìŠ¤íŠ¸ ===")
    answer = []
    # async for chunk in ai_search_answer_streaming(
    #     llm="gpt-4o-mini",
    #     prompt=answer_prompt.content,
    #     retrievers=json.dumps(retrievers, ensure_ascii=False)
    # ):
    #     answer.append(chunk.get('content', ''))
        
    print(answer)

if __name__ == "__main__":
    asyncio.run(test_answer_and_source())
