{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain_community.tools import TavilySearchResults\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from langchain.agents import tool\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def add(a: float, b: float):\n",
    "    \"\"\"Add two numbers.\"\"\"\n",
    "    return a + b\n",
    "\n",
    "@tool\n",
    "def multiply(a: float, b: float):\n",
    "    \"\"\"Multiply two numbers.\"\"\"\n",
    "    return a * b\n",
    "\n",
    "@tool\n",
    "def divide(a: float, b: float):\n",
    "    \"\"\"Divide two numbers.\"\"\"\n",
    "    return a / b\n",
    "\n",
    "\n",
    "# 계산기 툴 함수\n",
    "math_agent = create_react_agent(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    tools=[add, multiply, divide],\n",
    "    prompt=(\n",
    "        \"You are a math agent.\\n\\n\"\n",
    "        \"INSTRUCTIONS:\\n\"\n",
    "        \"- Assist ONLY with math-related tasks.\\n\"\n",
    "        \"- The user's question is given as input.\\n\"\n",
    "        \"- You MUST extract the exact numbers and operation from the question and perform ONLY that calculation.\\n\"\n",
    "        \"- DO NOT invent, generate, or solve any other math problems or examples.\\n\"\n",
    "        \"- If the question is ambiguous, ask for clarification.\\n\"\n",
    "        \"- After you're done with your task, respond to the supervisor directly.\\n\"\n",
    "        \"- Respond ONLY with the results of your work, do NOT include ANY other text.\\n\"\n",
    "        \"- 계산 결과를 반드시 메시지로 남겨\"\n",
    "    ),\n",
    "    name=\"math_agent\",\n",
    ")\n",
    "\n",
    "web_search = TavilySearchResults(k=3)\n",
    "\n",
    "research_agent = create_react_agent(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    tools=[web_search],\n",
    "    prompt=(\n",
    "        \"You are a research agent.\\n\\n\"\n",
    "        \"INSTRUCTIONS:\\n\"\n",
    "        \"- Assist ONLY with research-related tasks. DO NOT do any math.\\n\"\n",
    "        \"- The user's question is given as input.\\n\"\n",
    "        \"- You MUST use the web_search tool to answer ONLY the user's question.\\n\"\n",
    "        \"- DO NOT invent, generate, or search for any unrelated information.\\n\"\n",
    "        \"- After you're done with your task, respond to the supervisor directly.\\n\"\n",
    "        \"- Respond ONLY with the results of your work, do NOT include ANY other text.\"\n",
    "    ),\n",
    "    name=\"research_agent\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentExecutor, ReActChain\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0, openai_api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "tools = [add, multiply, divide]\n",
    "prompt = \"\"\"\n",
    "You are a math agent.\n",
    "\n",
    "INSTRUCTIONS:\n",
    "- Assist ONLY with math-related tasks.\n",
    "- The user's question is given as input.\n",
    "- You MUST extract the exact numbers and operation from the question and perform ONLY that calculation.\n",
    "- DO NOT invent, generate, or solve any other math problems or examples.\n",
    "- If the question is ambiguous, ask for clarification.\n",
    "- After you're done with your task, respond to the supervisor directly.\n",
    "- Respond ONLY with the results of your work as a message.\n",
    "\"\"\"\n",
    "\n",
    "# 1) create_react_agent로 에이전트 생성\n",
    "agent = create_react_agent(\n",
    "    llm, \n",
    "    tools, \n",
    "    prompt=prompt\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import convert_to_messages\n",
    "\n",
    "\n",
    "def pretty_print_message(message, indent=False):\n",
    "    pretty_message = message.pretty_repr(html=True)\n",
    "    if not indent:\n",
    "        print(pretty_message)\n",
    "        return\n",
    "\n",
    "    indented = \"\\n\".join(\"\\t\" + c for c in pretty_message.split(\"\\n\"))\n",
    "    print(indented)\n",
    "\n",
    "\n",
    "def pretty_print_messages(update, last_message=False):\n",
    "    is_subgraph = False\n",
    "    if isinstance(update, tuple):\n",
    "        ns, update = update\n",
    "        # skip parent graph updates in the printouts\n",
    "        if len(ns) == 0:\n",
    "            return\n",
    "\n",
    "        graph_id = ns[-1].split(\":\")[0]\n",
    "        print(f\"Update from subgraph {graph_id}:\")\n",
    "        print(\"\\n\")\n",
    "        is_subgraph = True\n",
    "\n",
    "    for node_name, node_update in update.items():\n",
    "        update_label = f\"Update from node {node_name}:\"\n",
    "        if is_subgraph:\n",
    "            update_label = \"\\t\" + update_label\n",
    "\n",
    "        print(update_label)\n",
    "        print(\"\\n\")\n",
    "\n",
    "        messages = convert_to_messages(node_update[\"messages\"])\n",
    "        if last_message:\n",
    "            messages = messages[-1:]\n",
    "\n",
    "        for m in messages:\n",
    "            pretty_print_message(m, indent=is_subgraph)\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update from node agent:\n",
      "\n",
      "\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  multiply (call_KLVQyiW6qxLjyA2tf5d3CIgN)\n",
      " Call ID: call_KLVQyiW6qxLjyA2tf5d3CIgN\n",
      "  Args:\n",
      "    a: 332\n",
      "    b: 7234\n",
      "  divide (call_P8Wu3k8byFYt02V0BYJOpFjt)\n",
      " Call ID: call_P8Wu3k8byFYt02V0BYJOpFjt\n",
      "  Args:\n",
      "    a: 332\n",
      "    b: 332\n",
      "\n",
      "\n",
      "Update from node tools:\n",
      "\n",
      "\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: divide\n",
      "\n",
      "1.0\n",
      "\n",
      "\n",
      "Update from node agent:\n",
      "\n",
      "\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  divide (call_PRyL3kEyJQeCuzykGAu0wySj)\n",
      " Call ID: call_PRyL3kEyJQeCuzykGAu0wySj\n",
      "  Args:\n",
      "    a: 2401688\n",
      "    b: 332\n",
      "\n",
      "\n",
      "Update from node tools:\n",
      "\n",
      "\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: divide\n",
      "\n",
      "7234.0\n",
      "\n",
      "\n",
      "Update from node agent:\n",
      "\n",
      "\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "7234.0\n",
      "\n",
      "\n",
      "[AIMessage(content='7234.0', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 5, 'prompt_tokens': 319, 'total_tokens': 324, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c', 'id': 'chatcmpl-Bghox5cf7sXlvuuQ7kxhQFaDOq4G7', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--5652bfd9-e885-4825-aec6-06d458305e5c-0', usage_metadata={'input_tokens': 319, 'output_tokens': 5, 'total_tokens': 324, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]\n"
     ]
    }
   ],
   "source": [
    "for chunk in agent.stream(\n",
    "        {\n",
    "            \"messages\": [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": \"한개의 수식 (332 곱하기 7234 의 결과를 나누기 332) 순서대로 계산했을 때 얼마야?\",\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "    ):\n",
    "        pretty_print_messages(chunk, last_message=True)\n",
    "\n",
    "final_message_history = chunk[\"agent\"][\"messages\"]\n",
    "print(final_message_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AIMessage(content='7234.0', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 5, 'prompt_tokens': 319, 'total_tokens': 324, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c', 'id': 'chatcmpl-Bghox5cf7sXlvuuQ7kxhQFaDOq4G7', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--5652bfd9-e885-4825-aec6-06d458305e5c-0', usage_metadata={'input_tokens': 319, 'output_tokens': 5, 'total_tokens': 324, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]\n"
     ]
    }
   ],
   "source": [
    "resutl =  agent.invoke(\n",
    "        {\n",
    "            \"messages\": [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": \"한개의 수식 (332 * 7234 / 332) 순서대로 계산했을 때 얼마야?\",\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "    )\n",
    "\n",
    "final_message_history1 = chunk[\"agent\"][\"messages\"]\n",
    "print(final_message_history1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langgraph_supervisor import create_supervisor\n",
    "from dotenv import load_dotenv\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# OpenAI API 키 환경변수로 설정 필요\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "supervisor = create_supervisor(\n",
    "    agents=[math_agent, research_agent],\n",
    "    model=init_chat_model(\"gpt-4o-mini\"),\n",
    "    prompt=(\n",
    "        \"너는 두 명의 에이전트를 관리하는 슈퍼바이저야:\\n\"\n",
    "        \"사용자의 질문을 분석하여 필요한 도구들을 순차적으로 사용해 정보를 얻고, 종합적인 답변을 생성하세요.\\n\"\n",
    "        \"사용자 입력: {input}\\n\"\n",
    "        \"- research agent: 정보 탐색 관련 질문만 할당\\n\"\n",
    "        \"- math agent: 수학 계산 관련 질문만 할당\\n\"\n",
    "        \"아래 input은 사용자의 질문이다.\\n\"\n",
    "        \"input의 질문을 반드시 읽고, 적절한 에이전트에게 바로 할당해서 답변을 받아 사용자에게 전달하라.\\n\"\n",
    "        \"수학 질문이면 반드시 질문에서 숫자와 연산을 정확히 추출해서 math agent에게 그 계산만 하도록 지시해야 한다.\\n\"\n",
    "        \"질문과 무관한 임의의 수식이나 추가 계산을 생성하거나 할당하지 마라.\\n\"\n",
    "        \"계산 요청이 들어오면, 오직 그 계산만 수행해야 한다.\\n\"\n",
    "        \"직접 답변하지 말고 반드시 agent의 결과를 사용자에게 전달하라.\\n\"\n",
    "    ),\n",
    "    add_handoff_back_messages=True,\n",
    "    output_mode=\"full_history\",\n",
    ").compile()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'IsLastStepManager' object has no attribute 'update'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[53]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msupervisor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrole\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m오늘 날짜 알려줘, 3 곱하기 7은 얼마야?\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpretty_print_messages\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlast_message\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m final_message_history = chunk[\u001b[33m\"\u001b[39m\u001b[33msupervisor\u001b[39m\u001b[33m\"\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\COM-TYUP\\miniforge3\\envs\\chatty2\\Lib\\site-packages\\langgraph\\pregel\\__init__.py:1778\u001b[39m, in \u001b[36mPregel.stream\u001b[39m\u001b[34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, subgraphs)\u001b[39m\n\u001b[32m   1772\u001b[39m     get_waiter = \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[32m   1773\u001b[39m \u001b[38;5;66;03m# Similarly to Bulk Synchronous Parallel / Pregel model\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;66;03m# computation proceeds in steps, while there are channel updates.\u001b[39;00m\n\u001b[32m   1775\u001b[39m \u001b[38;5;66;03m# Channel updates from step N are only visible in step N+1\u001b[39;00m\n\u001b[32m   1776\u001b[39m \u001b[38;5;66;03m# channels are guaranteed to be immutable for the duration of the step,\u001b[39;00m\n\u001b[32m   1777\u001b[39m \u001b[38;5;66;03m# with channel updates applied only at the transition between steps.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1778\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minput_channels\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m   1779\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m runner.tick(\n\u001b[32m   1780\u001b[39m         loop.tasks.values(),\n\u001b[32m   1781\u001b[39m         timeout=\u001b[38;5;28mself\u001b[39m.step_timeout,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1784\u001b[39m     ):\n\u001b[32m   1785\u001b[39m         \u001b[38;5;66;03m# emit output\u001b[39;00m\n\u001b[32m   1786\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m output()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\COM-TYUP\\miniforge3\\envs\\chatty2\\Lib\\site-packages\\langgraph\\pregel\\loop.py:427\u001b[39m, in \u001b[36mPregelLoop.tick\u001b[39m\u001b[34m(self, input_keys)\u001b[39m\n\u001b[32m    425\u001b[39m \u001b[38;5;66;03m# apply writes to managed values\u001b[39;00m\n\u001b[32m    426\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m key, values \u001b[38;5;129;01min\u001b[39;00m mv_writes.items():\n\u001b[32m--> \u001b[39m\u001b[32m427\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_update_mv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    428\u001b[39m \u001b[38;5;66;03m# produce values output\u001b[39;00m\n\u001b[32m    429\u001b[39m \u001b[38;5;28mself\u001b[39m._emit(\n\u001b[32m    430\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mvalues\u001b[39m\u001b[33m\"\u001b[39m, map_output_values, \u001b[38;5;28mself\u001b[39m.output_keys, writes, \u001b[38;5;28mself\u001b[39m.channels\n\u001b[32m    431\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\COM-TYUP\\miniforge3\\envs\\chatty2\\Lib\\site-packages\\langgraph\\pregel\\loop.py:891\u001b[39m, in \u001b[36mSyncPregelLoop._update_mv\u001b[39m\u001b[34m(self, key, values)\u001b[39m\n\u001b[32m    890\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_update_mv\u001b[39m(\u001b[38;5;28mself\u001b[39m, key: \u001b[38;5;28mstr\u001b[39m, values: Sequence[Any]) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m891\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.submit(\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mWritableManagedValue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmanaged\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupdate\u001b[49m, values)\n",
      "\u001b[31mAttributeError\u001b[39m: 'IsLastStepManager' object has no attribute 'update'"
     ]
    }
   ],
   "source": [
    "\n",
    "for chunk in supervisor.stream(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"오늘 날짜 알려줘, 3 곱하기 7은 얼마야?\",\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "):\n",
    "    pretty_print_messages(chunk, last_message=True)\n",
    "\n",
    "final_message_history = chunk[\"supervisor\"][\"messages\"]\n",
    "print(final_message_history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatty2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
